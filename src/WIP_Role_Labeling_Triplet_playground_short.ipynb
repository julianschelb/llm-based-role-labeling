{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to identify narratives in newspaper text is through considering the character archetypes relied on to compose the framing of an article. The main figures in an article may be represented as the heroes, villains, or victims in the text to guide the reader towards reading the article in context with existing qualities implicit in these character archetypes. Gomez-Zara et al present a dictionary-based method for computationally determining the hero, villain, and victim in a newspaper text, which Stammbach et al adapt by using an LLM for the same task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Articles (for Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials are sourced from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna-13B is an open-source chatbot developed by refining LLaMA through user-contributed conversations gathered from ShareGPT. Initial assessments employing GPT-4 as a referee indicate that Vicuna-13B attains over 90%* quality of OpenAI ChatGPT and Google Bard, surpassing other models such as LLaMA and Stanford Alpaca in over 90%* of instances. \n",
    "\n",
    "See:\n",
    "* https://github.com/lm-sys/FastChat\n",
    "* https://huggingface.co/lmsys/vicuna-13b-v1.5-16k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Start the controller service\n",
    "nohup python3 -m fastchat.serve.controller --host 0.0.0.0 --port 21001 &\n",
    "\n",
    "# Start the model_worker service\n",
    "nohup python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2 &\n",
    "#nohup python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-13b-v1.5 --num-gpus 2 &\n",
    "\n",
    "# Start the gradio_web_server service\n",
    "nohup python3 -m fastchat.serve.gradio_web_server --host 0.0.0.0 --port 7860 &\n",
    "\n",
    "# Launch the RESTful API server\n",
    "nohup python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 8080 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f'Number of available GPUs: {num_gpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List infos about the available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n",
      "GPU 1:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpu_info_list = listAvailableGPUs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 22 16:13:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    34W / 250W |  13224MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    36W / 250W |  13224MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1792350      C   ...ions/fastchat/bin/python3    13222MiB |\n",
      "|    1   N/A  N/A   1792350      C   ...ions/fastchat/bin/python3    13222MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x7ff06c896c00> JSON: {\n",
      "  \"index\": 0,\n",
      "  \"text\": \", we had a product that was the best in the market, and our customers\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7ff1b15ee020> JSON: {\n",
      "  \"index\": 1,\n",
      "  \"text\": \", there was a beautiful girl named Sophie. She had long, golden hair\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7ff1b15ee250> JSON: {\n",
      "  \"index\": 2,\n",
      "  \"text\": \", there was a little girl named Lily. She lived in a small town\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7ff1b15ee4d0> JSON: {\n",
      "  \"index\": 3,\n",
      "  \"text\": \", in a world not so different from our own, there was a kingdom ruled\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7ff1b15ee5c0> JSON: {\n",
      "  \"index\": 4,\n",
      "  \"text\": \", there was a little girl named Lily who lived in a small village surrounded\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "model = RemoteModel(model_name=\"vicuna-13b-v1.5-16k\",\n",
    "                    api_base=\"http://merkur72.inf.uni-konstanz.de:8080/v1\",\n",
    "                    api_key=\"EMPTY\")\n",
    "\n",
    "params = {'n': 5}\n",
    "result = model.generateAnswers(\"Once upon a time\", params=params)\n",
    "print(result, type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {hero: \"Name\", villain: \"Name\", victim: \"Name\"}. \n",
      " Headline: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Text: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Solution: \n"
     ]
    }
   ],
   "source": [
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. A hero is an individual, organisation, or entity admired for their courage, noble qualities, and outstanding achievements. A villain is a character, organisation, or entity known for their wickedness or malicious actions, often serving as an antagonist in a story or narrative. A victim is an individual, organisation, or entity who suffers harm or adversity, often due to an external force or action. Every entity can only be one of those roles. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Every entity can only be one of those roles. If not existing return None as name. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Each entity can only assume one role. If none apply, use 'None'. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "PROMPT_TEMPLATE = \"Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. \\n Headline: '{headline}' \\n Text: '{article_text}' \\n Solution: \"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE.format(headline = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.', article_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(prompt_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "        'early_stopping': True,\n",
    "        # 'max_length': 100,\n",
    "        # 'min_length': 1,\n",
    "        'logprobs': 1,\n",
    "        'n': 3,\n",
    "        #'best_of': 1,\n",
    "        \n",
    "        'num_beam_groups': 2,\n",
    "        'num_beams': 5,\n",
    "        'num_return_sequences': 5,\n",
    "        'max_tokens': 50,\n",
    "        'min_tokens': 0,\n",
    "        'output_scores': True,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'temperature': 0.6,\n",
    "        'top_k': 50,\n",
    "        'top_p': 1.0 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTriplet(answer):\n",
    "    \"\"\" Extracts the triplet from the answer string. \"\"\"\n",
    "    \n",
    "    # Extract keys and values using regex\n",
    "    keys = re.findall(r'(\\w+):\\s*\\\"', answer)\n",
    "    values = re.findall(r'\\\"(.*?)\\\"', answer)\n",
    "    result = dict(zip(keys, values))\n",
    "\n",
    "    if result == {}:    \n",
    "        keys = re.findall(r'(\\w+):\\s*([^,]+)', answer)\n",
    "        result = dict((k, v.strip('\"')) for k, v in keys)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswersTriplets(article, model, template, params):\n",
    "    \"\"\" Generates answers for the given article using the model and template. \"\"\"\n",
    "\n",
    "    # Extract the article headline and text\n",
    "    article_headline=article.get(\"title\", \"\")\n",
    "    article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "    # Generate the answer\n",
    "    prompt = template.format(headline = article_headline, article_text = article_text)\n",
    "    answers = model.generateAnswers(prompt, params)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(text, n_tokens, tokenizer, overlap=10):\n",
    "    \"\"\"Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, \n",
    "    with an overlap of overlap tokens from the previous and the next chunks.\"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    # No previous chunk at the beginning, so no need for overlap\n",
    "    chunks.append(tokenizer.convert_tokens_to_string(tokens[i:i+n_tokens]))\n",
    "    i += n_tokens\n",
    "\n",
    "    while i < len(tokens):\n",
    "        # Now, we include overlap from the previous chunk\n",
    "        start_index = i - overlap\n",
    "        end_index = start_index + n_tokens\n",
    "        chunk = tokens[start_index:end_index]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        i += n_tokens - overlap  # Moving the index to account for the next overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(articles, model, template, params, chunk_size=1024, overlap=256, show_progress=False, verbose=False):\n",
    "    \"\"\"Processes a batch of articles and extracts the triplets.\"\"\"\n",
    "    runtimes = []  # List to store the runtime for each article\n",
    "\n",
    "    # Iterate over the articles\n",
    "    for article in tqdm(articles, desc=\"Generating answers\", disable=not show_progress):\n",
    "        start_time = time.time()  # Start the timer\n",
    "\n",
    "        # Extract the article headline and text\n",
    "        article_headline = article.get(\"title\", \"\")\n",
    "        article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "        # Split the article text into chunks\n",
    "        chunks = splitText(article_text, chunk_size,\n",
    "                            model.tokenizer, overlap=overlap)\n",
    "\n",
    "        # print(\"Chunks:\", len(chunks))\n",
    "\n",
    "        chunk_results = []\n",
    "        for chunk_id, chunk in enumerate(chunks):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Chunk:\", chunk_id)\n",
    "                print(\"Chunk Length:\", calcInputLength(model.tokenizer, chunk))\n",
    "\n",
    "            prompt = template.format(headline=article_headline, article_text=chunk)\n",
    "            answers = model.generateAnswers(prompt, params)\n",
    "\n",
    "            # Extract the triplet from seach answer\n",
    "            for answer in answers:\n",
    "                answer[\"triplet\"] = extractTriplet(answer.get(\"text\"))\n",
    "\n",
    "            results = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk\": chunk,\n",
    "                \"answers\": answers\n",
    "            }\n",
    "            chunk_results.append(results)\n",
    "\n",
    "        article[\"triplets\"] = chunk_results\n",
    "\n",
    "        end_time = time.time()  # End the timer\n",
    "        runtime = end_time - start_time  # Calculate the runtime\n",
    "        runtimes.append(runtime)  # Store the runtime\n",
    "\n",
    "    return articles, runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticle(db, id: str, values: dict = {}, collection=\"articles\"):\n",
    "    \"Updates scraping task in database\"\n",
    "    filter = {\"_id\": ObjectId(id)}\n",
    "    values = {\"$set\": {**values}}\n",
    "    r = db[collection].update_one(filter, values)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticles(db, articles, collection = \"articles\"):\n",
    "    \"\"\"Updates the articles in the database.\"\"\"\n",
    "\n",
    "    for article in tqdm(articles, desc=\"Uploading results\"):\n",
    "        id = article.get(\"_id\")\n",
    "        values = {\"triplets\": article.get(\"triplets\", [])}\n",
    "        updateArticle(db, id, values, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100 # Number of articles to process in each batch\n",
    "CHUNK_SIZE = 20_000 # Number of tokens in each chunk\n",
    "OVERLAP = 64 # Number of overlapping tokens between chunks\n",
    "COLLECTION = \"articles.sampled.triplets\"\n",
    "\n",
    "FIELDS = {\"url\": 1, \"title\": 1, \"parsing_result.text\": 1}\n",
    "QUERY = {\"triplets\": {\"$exists\": False}, \n",
    "         \"parsing_result.text_length\": {\"$lt\": 1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Batch 0 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   1%|          | 1/100 [00:13<21:38, 13.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Process the batch of articles\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m articles, runtimes \u001b[39m=\u001b[39m processBatch(articles, model, PROMPT_TEMPLATE, params, chunk_size\u001b[39m=\u001b[39;49mCHUNK_SIZE, overlap\u001b[39m=\u001b[39;49mOVERLAP, show_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Update the articles in the database\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m updateArticles(db, articles, COLLECTION)\n",
      "\u001b[1;32m/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChunk Length:\u001b[39m\u001b[39m\"\u001b[39m, calcInputLength(model\u001b[39m.\u001b[39mtokenizer, chunk))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m prompt \u001b[39m=\u001b[39m template\u001b[39m.\u001b[39mformat(headline\u001b[39m=\u001b[39marticle_headline, article_text\u001b[39m=\u001b[39mchunk)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m answers \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerateAnswers(prompt, params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Extract the triplet from seach answer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmerkur72.inf.uni-konstanz.de/home/jschelb/GenModel-Experiments-clean/src/WIP_Role_Labeling_Triplet_playground_short.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m answer \u001b[39min\u001b[39;00m answers:\n",
      "File \u001b[0;32m~/GenModel-Experiments-clean/src/utils/model.py:87\u001b[0m, in \u001b[0;36mRemoteModel.generateAnswers\u001b[0;34m(self, input_text, params)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate an answer using the remote API.\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[39m# Generate the output using the model\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     88\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     89\u001b[0m         prompt\u001b[39m=\u001b[39;49minput_text,\n\u001b[1;32m     90\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     93\u001b[0m     \u001b[39mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"------ Batch {batch_id} ------\")\n",
    "\n",
    "    # Fetch the next batch of articles\n",
    "    articles = fetchArticleTexts(db, LIMIT, 0, FIELDS, QUERY, COLLECTION)\n",
    "    \n",
    "    # Stop if no more articles are available\n",
    "    if not articles:\n",
    "        break\n",
    "    \n",
    "    # Process the batch of articles\n",
    "    articles, runtimes = processBatch(articles, model, PROMPT_TEMPLATE, params, chunk_size=CHUNK_SIZE, overlap=OVERLAP, show_progress=True)\n",
    "\n",
    "    # Update the articles in the database\n",
    "    updateArticles(db, articles, COLLECTION)\n",
    "    print(f\"Updated {len(articles)} articles\", end=\"\\n\\n\")\n",
    "\n",
    "    batch_id += 1\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
