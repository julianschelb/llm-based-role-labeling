{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to identify narratives in newspaper text is through considering the character archetypes relied on to compose the framing of an article. The main figures in an article may be represented as the heroes, villains, or victims in the text to guide the reader towards reading the article in context with existing qualities implicit in these character archetypes. Gomez-Zara et al present a dictionary-based method for computationally determining the hero, villain, and victim in a newspaper text, which Stammbach et al adapt by using an LLM for the same task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Articles (for Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials are sourced from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna-13B is an open-source chatbot developed by refining LLaMA through user-contributed conversations gathered from ShareGPT. Initial assessments employing GPT-4 as a referee indicate that Vicuna-13B attains over 90%* quality of OpenAI ChatGPT and Google Bard, surpassing other models such as LLaMA and Stanford Alpaca in over 90%* of instances. \n",
    "\n",
    "See:\n",
    "* https://github.com/lm-sys/FastChat\n",
    "* https://huggingface.co/lmsys/vicuna-13b-v1.5-16k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Start the controller service\n",
    "nohup python3 -m fastchat.serve.controller --host 0.0.0.0 --port 21001 &\n",
    "\n",
    "# Start the model_worker service\n",
    "nohup python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2 &\n",
    "\n",
    "# Start the gradio_web_server service\n",
    "nohup python3 -m fastchat.serve.gradio_web_server --host 0.0.0.0 --port 7860 &\n",
    "\n",
    "# Launch the RESTful API server\n",
    "nohup python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 8080 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", there was a young girl named Sophie. Sophie lived in a small\n"
     ]
    }
   ],
   "source": [
    "model = RemoteModel(model_name=\"vicuna-13b-v1.5-16k\",\n",
    "                    api_base=\"http://merkur72.inf.uni-konstanz.de:8080/v1\",\n",
    "                    api_key=\"EMPTY\")\n",
    "\n",
    "print(model.generateAnswer(\"Once upon a time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {hero: \"Name\", villain: \"Name\", victim: \"Name\"}. \n",
      " Headline: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Text: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Solution: \n"
     ]
    }
   ],
   "source": [
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. A hero is an individual, organisation, or entity admired for their courage, noble qualities, and outstanding achievements. A villain is a character, organisation, or entity known for their wickedness or malicious actions, often serving as an antagonist in a story or narrative. A victim is an individual, organisation, or entity who suffers harm or adversity, often due to an external force or action. Every entity can only be one of those roles. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Every entity can only be one of those roles. If not existing return None as name. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Each entity can only assume one role. If none apply, use 'None'. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "PROMPT_TEMPLATE = \"Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. \\n Headline: '{headline}' \\n Text: '{article_text}' \\n Solution: \"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE.format(headline = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.', article_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(prompt_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "        'early_stopping': True,\n",
    "        # 'max_length': 100,\n",
    "        # 'min_length': 1,\n",
    "        'logprobs': 1,\n",
    "        'n': 1,\n",
    "        'best_of': 1,\n",
    "        \n",
    "        'num_beam_groups': 2,\n",
    "        'num_beams': 5,\n",
    "        'num_return_sequences': 5,\n",
    "        'max_tokens': 50,\n",
    "        'min_tokens': 0,\n",
    "        'output_scores': True,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'temperature': 0.6,\n",
    "        'top_k': 50,\n",
    "        'top_p': 1.0 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTriplet(answer):\n",
    "    \"\"\" Extracts the triplet from the answer string. \"\"\"\n",
    "    \n",
    "    # Extract keys and values using regex\n",
    "    keys = re.findall(r'(\\w+):\\s*\\\"', answer)\n",
    "    values = re.findall(r'\\\"(.*?)\\\"', answer)\n",
    "    result = dict(zip(keys, values))\n",
    "\n",
    "    if result == {}:    \n",
    "        keys = re.findall(r'(\\w+):\\s*([^,]+)', answer)\n",
    "        result = dict((k, v.strip('\"')) for k, v in keys)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswersTriplets(article, model, template, params):\n",
    "    \"\"\" Generates answers for the given article using the model and template. \"\"\"\n",
    "\n",
    "    # Extract the article headline and text\n",
    "    article_headline=article.get(\"title\", \"\")\n",
    "    article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "    # Generate the answer\n",
    "    prompt = template.format(headline = article_headline, article_text = article_text)\n",
    "    answer = model.generateAnswer(prompt, params)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(text, n_tokens, tokenizer, overlap=10):\n",
    "    \"\"\"Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, \n",
    "    with an overlap of overlap tokens from the previous and the next chunks.\"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    # No previous chunk at the beginning, so no need for overlap\n",
    "    chunks.append(tokenizer.convert_tokens_to_string(tokens[i:i+n_tokens]))\n",
    "    i += n_tokens\n",
    "\n",
    "    while i < len(tokens):\n",
    "        # Now, we include overlap from the previous chunk\n",
    "        start_index = i - overlap\n",
    "        end_index = start_index + n_tokens\n",
    "        chunk = tokens[start_index:end_index]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        i += n_tokens - overlap  # Moving the index to account for the next overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(articles, model, template, params, chunk_size=1024, overlap=256, show_progress=False, verbose=False):\n",
    "    \"\"\"Processes a batch of articles and extracts the triplets.\"\"\"\n",
    "    runtimes = []  # List to store the runtime for each article\n",
    "\n",
    "    # Iterate over the articles\n",
    "    for article in tqdm(articles, desc=\"Generating answers\", disable=not show_progress):\n",
    "        start_time = time.time()  # Start the timer\n",
    "\n",
    "        # Extract the article headline and text\n",
    "        article_headline = article.get(\"title\", \"\")\n",
    "        article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "        # Split the article text into chunks\n",
    "        chunks = splitText(article_text, chunk_size,\n",
    "                            model.tokenizer, overlap=overlap)\n",
    "\n",
    "        # print(\"Chunks:\", len(chunks))\n",
    "\n",
    "        chunk_results = []\n",
    "        for chunk_id, chunk in enumerate(chunks):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Chunk:\", chunk_id)\n",
    "                print(\"Chunk Length:\", calcInputLength(model.tokenizer, chunk))\n",
    "            # print(\"Headline Length:\", calcInputLength(\n",
    "            #     tokenizer, article_headline))\n",
    "\n",
    "            prompt = template.format(\n",
    "                headline=article_headline, article_text=chunk)\n",
    "            answer = model.generateAnswer(prompt, params)\n",
    "            triplet = extractTriplet(answer)\n",
    "\n",
    "            # print(chunk_id, \"Answer:\", triplet, \"Type:\", type(triplet))\n",
    "            results = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk\": chunk,\n",
    "                \"answer\": answer,\n",
    "                \"triplet\": triplet\n",
    "            }\n",
    "            chunk_results.append(results)\n",
    "\n",
    "        article[\"triplets\"] = chunk_results\n",
    "\n",
    "        end_time = time.time()  # End the timer\n",
    "        runtime = end_time - start_time  # Calculate the runtime\n",
    "        runtimes.append(runtime)  # Store the runtime\n",
    "\n",
    "    return articles, runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticle(db, id: str, values: dict = {}, collection=\"articles\"):\n",
    "    \"Updates scraping task in database\"\n",
    "    pass # TODO: Uncomment to update the database\n",
    "    # filter = {\"_id\": ObjectId(id)}\n",
    "    # values = {\"$set\": {**values}}\n",
    "    # r = db[collection].update_one(filter, values)\n",
    "    # return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticles(db, articles):\n",
    "    \"\"\"Updates the articles in the database.\"\"\"\n",
    "\n",
    "    for article in tqdm(articles, desc=\"Uploading results\"):\n",
    "        id = article.get(\"_id\")\n",
    "        values = {\"triplets\": article.get(\"triplets\", [])}\n",
    "        #updateArticle(db, id, values) # TODO: Uncomment to update the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"articles\"\n",
    "fields = {\"url\": 1, \"title\": 1, \"parsing_result.text\": 1}\n",
    "query = {\"processing_result\": {\"$exists\": False}, \n",
    "         \"parsing_result.text_length\": {\"$lt\": 10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10 # Number of articles to process in each batch\n",
    "CHUNK_SIZE = 1024 # Number of tokens in each chunk\n",
    "OVERLAP = 64 # Number of overlapping tokens between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Batch 0 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 10/10 [01:08<00:00,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 10 articles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"------ Batch {batch_id} ------\")\n",
    "\n",
    "    # Fetch the next batch of articles\n",
    "    articles = fetchArticleTexts(db, LIMIT, 0, fields, query)\n",
    "    \n",
    "    # Stop if no more articles are available\n",
    "    if not articles:\n",
    "        break\n",
    "    \n",
    "    # Process the batch of articles\n",
    "    articles, runtimes = processBatch(articles, model, PROMPT_TEMPLATE, params, chunk_size=CHUNK_SIZE, overlap=OVERLAP, show_progress=True, verbose=False)\n",
    "\n",
    "    # Update the articles in the database\n",
    "    #updateArticles(db, articles)\n",
    "    print(f\"Updated {len(articles)} articles\", end=\"\\n\\n\")\n",
    "\n",
    "    batch_id += 1\n",
    "    break # TODO: Remove to process all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
