{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Entity Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilise GENRE to disambiguate the answers acquired from the first stage 1. GENRE utilizes a sequence-to-sequence approach for entity retrieval, like linking, built on a fine-tuned BART architecture 2. GENRE produces the unique entity name based on the provided input text. Entities intended for disambiguation are marked in the prompt using special tokens: START_ENT and END_ENT. For merging the entity with the article text for more context, we use the following prompt template:\n",
    "\n",
    "```text\n",
    "Discussing [START_ENT] ([HERO], [VILLAIN] or [VICTIM]) [END_ENT]: [TEXT]. \n",
    "```\n",
    "\n",
    "De Cao et al. originally combined a prefix tree with a constrained beam search to exclusively generate identifiers corresponding to Wikipedia titles. In our context, given that many entities, such as lesser-known individuals, might not have a Wikipedia page, we employed GENRE without the prefix tree constraint. This approach refined the answers derived from the FLAN model. For high-profile entities, like politicians, the model was able to disambiguate different spelling variants ( Example: D. J. Trump to Donald Trump). Additionally, our observations indicated that when GENRE was used in this manner, it yielded a less noisy output, stripping away redundant punctuation, resolving abbreviations, and at times retrieving information from the original article text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials are sourced from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches a limited number of articles from the database that haven't been processed yet, \n",
    "returning specified fields like url, title, and parsing result text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"v2_sampled_articles\"\n",
    "fields = {\"url\": 1, \"title\": 1, \"parsing_result.text\": 1, 'processing_result': 1}\n",
    "query = {\n",
    "    \"processing_result\": {\"$exists\": True},\n",
    "    \"parsing_result.text_length\": {\"$lt\": 10000},\n",
    "    #\"denoising_result\": {\"$exists\": False},\n",
    "    #\"initial_subsample\": False\n",
    "}\n",
    "articles = fetchArticleTexts(db, 50, 0, fields, query, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The New Civil Rights Movement\n",
      "Text: Secretary of Housing and Urban Development Dr. Ben Carson, is the latest member of the Trump administration to test positive for coronavirus. CNBC reports the news, after... \"This sounded like a slur to me.\" \"But now, fortunately, God's given me a chance to do something about it,\" Carson added. \"We are all now more stupid than we were when we came in the room today sir, thank you,\" Rep. Quigley concluded after going around and... If the government were not shutdown, taxpayers would be on the hook for the Secretary's travel to Missouri. 'It's a Very Complex Issue' Carson Claims Newly released emails show Ben Carson and his wife personally selected a $31,000 dining room set for his office at the Department of Housing and Urban... 'Sometimes I Get a Little Bit Tired of People Ascribing to Me Things That People Have Said That I Believe' Carson Says Here's What You'll Want to Know About Carson's Name Mysteriously Disappears From Schedule HUD Secretary Ben Carson Under Fire for Calling African-American Slaves 'Immigrants' (Video) 10 Democrats Vote to Advance Debate Carson Has Previously Called Equality for LGBT People in Marriage and Other Areas 'Extra Rights' Carson Not Qualified to Head HUD Huckabee Must Also Think Dr. Carson Is 'Racist or Just Dumb' Too Copyright © 2020 AlterNet Media.\n",
      "Processing Result: {'hero': [], 'villain': ['Rep. Quigley'], 'victim': ['Dr. Ben Carson']}\n"
     ]
    }
   ],
   "source": [
    "example_article = random.choice(articles)\n",
    "title = example_article.get(\"title\")\n",
    "text = example_article.get(\"parsing_result\").get(\"text\")\n",
    "print(f\"Title: {title}\\nText: {text}\")\n",
    "print(f\"Processing Result: {example_article.get('processing_result')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Processes the 'parsing_result' of each article to clean the text, and filters out articles \n",
    "that lack a 'title' or 'parsing_result'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning articles: 100%|██████████| 50/50 [00:00<00:00, 1599.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Basic text cleaning, e.g. removing newlines, tabs, etc.\n",
    "articles = cleanArticles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 50\n"
     ]
    }
   ],
   "source": [
    "# Filter out articles with no title or no parsing result \n",
    "articles = [article for article in articles if article.get(\n",
    "    \"title\", \"\") and article.get(\"parsing_result\", \"\")]\n",
    "\n",
    "print(\"Number of articles:\", len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the given data to a JSON file for optional visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportAsJSON(\"../data/input/articles.json\",  articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to HF Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert article IDs to strings and transform a list of articles into a dataset with fields: id, title, url, and text extracted from parsing results. The HuggingFace `datasets` library provides several key advantages over plain JSON files:\n",
    "\n",
    "- **Efficiency**: The datasets are memory-mapped, allowing you to work with data that's larger than your available RAM without loading the entire dataset into memory. \n",
    "- **Speed**: Datasets in the HuggingFace format (which is Arrow-based) can be loaded faster than large JSON files, facilitating quicker data operations.\n",
    "- **Columnar Storage**: By using Apache Arrow for storage, HuggingFace datasets benefit from a columnar format that ensures more efficient serialization and deserialization compared to row-based storage, such as JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 50\n",
      "Column names: ['_id', 'title', 'url', 'text', 'hero', 'villain', 'victim']\n",
      "Features (schema): {'_id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'hero': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'villain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'victim': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"_id\", \"title\", \"url\", \"parsing_result.text\", \"processing_result.hero\", \"processing_result.villain\", \"processing_result.victim\"]\n",
    "articles = convertListToDataset(articles, column_names)\n",
    "describeDataset(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Article Text: UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a fed\n",
      "Example Article Hero: ['David Ward']\n",
      "Example Article Villain: ['Ammon Bundy']\n",
      "Example Article Victim: ['Harney County residents']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Article Text:\", articles[42][\"text\"][:100])\n",
    "print(\"Example Article Hero:\", articles[42][\"hero\"])\n",
    "print(\"Example Article Villain:\", articles[42][\"villain\"])\n",
    "print(\"Example Article Victim:\", articles[42][\"victim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 50/50 [00:00<00:00, 5860.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "articles.save_to_disk('../data/input/articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from multiprocessing import Pool\n",
    "from utils.preprocessing import *\n",
    "from utils.database import *\n",
    "from utils.files import *\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code `os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"` disables parallel tokenization in HuggingFace's libraries. It's a way to suppress warnings and prevent potential issues tied to multi-core tokenization.\n",
    "See: https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 50\n",
      "Column names: ['_id', 'title', 'url', 'text', 'hero', 'villain', 'victim']\n",
      "Features (schema): {'_id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'hero': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'villain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'victim': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "articles = load_from_disk('../data/input/articles')\n",
    "describeDataset(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussing [START_ENT] Donald Trump [END_ENT] . Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE_prefix = \"Discussing [START_ENT] {entity} [END_ENT]\"\n",
    "PROMPT_TEMPLATE = \"{prefix} . {article}\"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prefix = PROMPT_TEMPLATE_prefix.format(entity='Donald Trump')\n",
    "print(PROMPT_TEMPLATE.format(prefix=prefix,\n",
    "      article='Lorem ipsum dolor sit amet, consectetur adipiscing elit.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to segment articles into chunks fitting within the input window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input window length: 1024\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-kilt\", add_prefix_space=True)\n",
    "print(\"Input window length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of empty prompt template: 20\n"
     ]
    }
   ],
   "source": [
    "template_length = calcInputLength(tokenizer, PROMPT_TEMPLATE.format(prefix=prefix, article=' '))\n",
    "print(\"Max length of empty prompt template:\", template_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each article, distinct prompts identify 'hero', 'villain', and 'victim'. If an article exceeds the model's input size, it's divided into chunks, generating additional prompts. It seems that one article results in about 10 to 12 prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '64d8eb39516b265872292f80',\n",
       " 'title': 'Oregon Sheriff To Meet Again Friday With Armed Refuge Militia',\n",
       " 'url': 'http://talkingpointsmemo.com/livewire/militia-sheriff-peaceful-resolution--2',\n",
       " 'text': 'UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019',\n",
       " 'hero': ['David Ward'],\n",
       " 'villain': ['Ammon Bundy'],\n",
       " 'victim': ['Harney County residents']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandRow(row, template_prefix, template, col_name=\"text\", roles=['hero', 'villain', 'victim']):\n",
    "    \"\"\"\n",
    "    Generate prompts based on various roles and text chunks from the input row.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "\n",
    "    # Generate prompts for each role and text chunk\n",
    "    for role in roles:\n",
    "        for entity in row.get(role, []):\n",
    "            text = row.get(col_name, \"\")\n",
    "            prompt_prefix = template_prefix.format(entity=entity)\n",
    "            prompt = template.format(\n",
    "                prefix=prompt_prefix, article=text)\n",
    "            new_row = {\n",
    "                **row,\n",
    "                'prompt': prompt,\n",
    "                'role': role,\n",
    "                'entity': entity,\n",
    "            }\n",
    "            prompts.append(new_row)\n",
    "            \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt: Discussing [START_ENT] David Ward [END_ENT] . UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019\n",
      "Expanded row: [{'_id': '64d8eb39516b265872292f80', 'title': 'Oregon Sheriff To Meet Again Friday With Armed Refuge Militia', 'url': 'http://talkingpointsmemo.com/livewire/militia-sheriff-peaceful-resolution--2', 'text': 'UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019', 'hero': ['David Ward'], 'villain': ['Ammon Bundy'], 'victim': ['Harney County residents'], 'prompt': 'Discussing [START_ENT] David Ward [END_ENT] . UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019', 'role': 'hero', 'entity': 'David Ward'}, {'_id': '64d8eb39516b265872292f80', 'title': 'Oregon Sheriff To Meet Again Friday With Armed Refuge Militia', 'url': 'http://talkingpointsmemo.com/livewire/militia-sheriff-peaceful-resolution--2', 'text': 'UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019', 'hero': ['David Ward'], 'villain': ['Ammon Bundy'], 'victim': ['Harney County residents'], 'prompt': 'Discussing [START_ENT] Ammon Bundy [END_ENT] . UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019', 'role': 'villain', 'entity': 'Ammon Bundy'}, {'_id': '64d8eb39516b265872292f80', 'title': 'Oregon Sheriff To Meet Again Friday With Armed Refuge Militia', 'url': 'http://talkingpointsmemo.com/livewire/militia-sheriff-peaceful-resolution--2', 'text': 'UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019', 'hero': ['David Ward'], 'villain': ['Ammon Bundy'], 'victim': ['Harney County residents'], 'prompt': 'Discussing [START_ENT] Harney County residents [END_ENT] . UPDATE: 6:52 p.m. The sheriff of the Oregon county where armed militia members have taken over a federal wildlife refuge met Thursday afternoon local time with the militia members and will meet again Friday. Here’s the update his office tweeted after the Thursday the discussion: Meeting now over. Plans to talk again tomorrow. Sheriff Ward called on them for a peaceful resolution. #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 The office also stated that Harney County Sheriff David Ward asked Ammon Bundy, one of the men leading the militia takeover, to leave. Sheriff Ward asks Ammon Bundy to please leave and respect the wishes of Harney County residents. — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 His office emphasized he is seeking a “peaceful resolution” and is not there to arrest anyone in a tweet. This meeting is called for a peaceful resolution. Sheriff Ward is NOT there to make an arrest. He’s there to ask them to leave #HarneyCounty — Harney Cty. Sheriff (@HarneyCoSheriff) January 7, 2016 Ward had previously said those who had taken over the refuge would face charges “at some point.” A local TV reporter tweeted a photo of the meeting: Bundy and sheriff meet in high Oregon desert pic.twitter.com/w79c9IMdWx — Pat Dooris (@PatDooris) January 7, 2016 Planned Parenthood is leaving the federal government’s Title X program, which funds family planning services for low-income people, due to… Following the ousters of the Homeland Security secretary and Secret Service director, and the withdrawal of the would-be ICE director’s… Senate Minority Leader Chuck Schumer (D-NY) on Monday called for ousted Secret Service Director Randolph “Tex” Alles to testify before… Nielsen: \"I just want to thank the President, again.\" pic.twitter.com/5m8WSU9Xej — TPM Livewire (@TPMLiveWire) April 8, 2019', 'role': 'victim', 'entity': 'Harney County residents'}]\n",
      "Expanded row length: 3\n"
     ]
    }
   ],
   "source": [
    "roles=['hero', 'villain', 'victim']\n",
    "col_name=\"text\"\n",
    "\n",
    "example_row = articles[42]\n",
    "example_row_exp = expandRow(example_row, PROMPT_TEMPLATE_prefix, PROMPT_TEMPLATE, col_name, roles)\n",
    "\n",
    "print(\"Example Prompt:\", example_row_exp[0].get(\"prompt\"))\n",
    "print(\"Expanded row:\", example_row_exp)\n",
    "print(\"Expanded row length:\", len(example_row_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process datataset using multiple proesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Discussing [START_ENT] {entity} [END_ENT]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_TEMPLATE_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 12\n",
    "params = (PROMPT_TEMPLATE_prefix, PROMPT_TEMPLATE, col_name, roles,)\n",
    "dataset_hvv = processDataset(articles, num_processes, expandRow, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 109/109 [00:00<00:00, 5859.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_hvv.save_to_disk('../data/input/articles_chunkified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Dataset\n",
    "\n",
    "Tokenization refers to the process of converting input text into smaller units, such as words or subwords, which are then represented as tokens. These tokens are mapped to indices in a vocabulary that the model can understand. Hugging Face provides a variety of tokenizers, each suited for different types of models. For instance, the BertTokenizer is designed for BERT-like models and tokenizes text into wordpieces. Similarly, the GPT2Tokenizer is tailored for GPT-2-like models and tokenizes text into subwords using the Byte-Pair Encoding (BPE) algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "Below are descriptions of key parameters helpful for using these tokenizers:\n",
    "\n",
    "**`add_special_tokens`**:\n",
    "* Whether to add special tokens such as `[CLS]` and `[SEP]` (default is True).\n",
    "* Special tokens are necessary for some models to function properly.\n",
    "\n",
    "**`max_length`**:\n",
    "* The maximum number of tokens for the output (default varies, often 512).\n",
    "* Texts longer than this will be truncated.\n",
    "\n",
    "**`padding`**:\n",
    "* Whether to pad the output to `max_length`, and the padding strategy (default is False).\n",
    "* Options include `'max_length'`, `'longest'`, or `True` to pad to the length of the longest sequence.\n",
    "\n",
    "**`truncation`**:\n",
    "* Whether to truncate sequences to `max_length` (default is False).\n",
    "\n",
    "**`return_tensors`**:\n",
    "* The framework to use for the returned tensors, either `'pt'` for PyTorch or `'tf'` for TensorFlow (default is None, which returns plain lists).\n",
    "\n",
    "**`return_token_type_ids`**:\n",
    "* Whether to return token type IDs (default is True).\n",
    "* Necessary for some models to understand the different segments of input (e.g., question vs answer).\n",
    "\n",
    "**`return_attention_mask`**:\n",
    "* Whether to return the attention mask (default is True).\n",
    "* Attention masks tell the model which tokens to pay attention to and which to ignore.\n",
    "\n",
    "**`verbose`**:\n",
    "* Whether to log information during tokenization (default is True).\n",
    "\n",
    "**`is_split_into_words`**:\n",
    "* Whether the input is pre-tokenized into words (default is False).\n",
    "\n",
    "These parameters allow for fine-grained control over the tokenization process, ensuring the text is prepared in a way that's suitable for your model and task.\n",
    "\n",
    "For more information, consider checking the [`encode` and `encode_plus` methods documentation](https://huggingface.co/transformers/main_classes/tokenizer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 109/109 [00:00<00:00, 280.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Paramater passed to the tokenizer\n",
    "tokenizer_params = {\"truncation\": True, \"is_split_into_words\": False,\n",
    "                    \"add_special_tokens\": True, \"padding\": \"max_length\"}\n",
    "\n",
    "# Parameters passed to the tokenization function\n",
    "params = {\"tokenizer\": tokenizer, \"col_name\": \"prompt\", \"params\": tokenizer_params}\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset_hvv.map(tokenizeInputs, fn_kwargs=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 109/109 [00:00<00:00, 9280.19 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk('../data/input/articles_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 109\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk('../data/input/articles_tokenized')\n",
    "print(\"Dataset length:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List infos about the available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n",
      "GPU 1:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpu_info_list = listAvailableGPUs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f'Number of available GPUs: {num_gpus}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2\n",
      "Chunk 0 length: 55\n",
      "Chunk 1 length: 54\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into chunks (one for each GPU)\n",
    "chunks = splitDataset(dataset, num_chunks=num_gpus)\n",
    "\n",
    "# Print the length of each chunk\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i} length:\", len(chunk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 22 09:57:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    27W / 250W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    29W / 250W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for Text Generation\n",
    "\n",
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePredictions(process_id, dataset, device):\n",
    "    \"\"\"Generates predictions for a given dataset.\"\"\"\n",
    "\n",
    "    # Print some information about the process\n",
    "    print(f\"--------- Process {process_id:02} ---------\")\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"------------------------------\")\n",
    "\n",
    "    # Load tokenizer and model for generation\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"facebook/genre-kilt\", add_prefix_space=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-kilt\").eval()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Device:\", torch.cuda.get_device_name())\n",
    "\n",
    "    dataset_full = copy.copy(dataset)\n",
    "    dataset.set_format(type='torch', columns=[\n",
    "        'input_ids', 'attention_mask'])\n",
    "\n",
    "    # Create dataloader without explicit sampler for sequential loading\n",
    "    BATCH_SIZE = 64\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    params = {'do_sample': True,\n",
    "              'early_stopping': False,\n",
    "              # 'max_length': 100,\n",
    "              # 'min_length': 1,\n",
    "              # 'num_beam_groups': 2,\n",
    "              # 'num_beams': 5,\n",
    "              # 'max_tokens': 32,\n",
    "              # 'min_tokens': 1,\n",
    "              # 'output_scores': False,\n",
    "              # 'num_return_sequences': 1,\n",
    "              'repetition_penalty': 1.0,\n",
    "              # 'return_dict_in_generate': False,\n",
    "              'temperature': 1.0,\n",
    "              'top_k': 50,\n",
    "              'top_p': 1.0, }\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Batches\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Generate outputs\n",
    "            batch_outputs = model.generate(\n",
    "                input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], **params, max_new_tokens=10)\n",
    "\n",
    "            # Decode and store predictions\n",
    "            decoded_outputs = [tokenizer.decode(\n",
    "                output_id, skip_special_tokens=True) for output_id in batch_outputs]\n",
    "            predictions.extend(decoded_outputs)\n",
    "\n",
    "    # results.extend(predictions)\n",
    "\n",
    "    # Ensure the new column has the same number of items as the dataset\n",
    "    assert len(dataset_full) == len(\n",
    "        predictions), \"The length of new_column_values must match the dataset's length\"\n",
    "\n",
    "    # Add new column\n",
    "    dataset_full = dataset_full.add_column('answer', predictions)\n",
    "    dataset_full.save_to_disk('data/output/articles_processed_' + str(id))\n",
    "\n",
    "    return dataset_full\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start one thread per GPU before collecting and merging the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Process 00 ---------\n",
      "Dataset length: 55\n",
      "Device: cuda:0\n",
      "------------------------------\n",
      "--------- Process 01 ---------\n",
      "Dataset length: 54\n",
      "Device: cuda:1\n",
      "------------------------------\n",
      "Device: Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.27s/it] [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54/54 [00:00<00:00, 2474.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 55/55 [00:00<00:00, 2271.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of returned datasets: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 109/109 [00:00<00:00, 9289.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing on both GPUs completed!\n",
      "Results: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming datasets and devices are lists containing the datasets and device names\n",
    "datasets = chunks  # and so on...\n",
    "devices = ['cuda:0', 'cuda:1']  # and so on...\n",
    "\n",
    "# Calls the function to start the threads\n",
    "returned_datasets = startThreads(len(datasets), datasets, devices, generatePredictions)\n",
    "print(\"Number of returned datasets:\", len(returned_datasets))\n",
    "\n",
    "# Concatenate the returned datasets\n",
    "merged_dataset = concatenate_datasets(returned_datasets)\n",
    "merged_dataset.save_to_disk('../data/output/articles_processed')\n",
    "\n",
    "# Print the length of the merged dataset\n",
    "print(\"Processing on both GPUs completed!\")\n",
    "print(\"Results:\", len(merged_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'url', 'text', 'hero', 'villain', 'victim', 'prompt', 'role', 'entity', 'input_ids', 'attention_mask', 'answer'],\n",
       "    num_rows: 109\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = merged_dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import *\n",
    "from utils.database import *\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 109\n",
      "Column names: ['_id', 'title', 'url', 'text', 'hero', 'villain', 'victim', 'prompt', 'role', 'entity', 'input_ids', 'attention_mask', 'answer']\n",
      "Features (schema): {'_id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'hero': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'villain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'victim': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'prompt': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None), 'entity': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'answer': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk('../data/output/articles_processed')\n",
    "describeDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: villain\n",
      "Answer: List of United States Republican Party presidential candidates\n"
     ]
    }
   ],
   "source": [
    "# Exmample amswer \n",
    "answer = dataset[0].get(\"answer\")\n",
    "role = dataset[0].get(\"role\")\n",
    "print(\"Role:\", role)\n",
    "print(\"Answer:\", answer)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Documents in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processResults(dataset):\n",
    "    # Initial processing results\n",
    "    processing_result = {\"hero\": [], \"villain\": [], \"victim\": []}\n",
    "    object_id_prev = None\n",
    "\n",
    "    for item in dataset:\n",
    "        object_id = item['_id']\n",
    "        role = item['role']\n",
    "        answer = item['answer']\n",
    "\n",
    "        # If the object_id changes, reset the processing_result\n",
    "        if object_id_prev is not None and object_id_prev != object_id:\n",
    "            yield object_id_prev, processing_result\n",
    "            processing_result = {\"hero\": [], \"villain\": [], \"victim\": []}\n",
    "\n",
    "        processing_result[role].append(answer)\n",
    "        object_id_prev = object_id\n",
    "\n",
    "    # Yield the final processing_result if any\n",
    "    if processing_result[\"hero\"] or processing_result[\"villain\"] or processing_result[\"victim\"]:\n",
    "        yield object_id_prev, processing_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading results: 100%|██████████| 48/48 [00:00<00:00, 256.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assuming `ds` is your dataset object\n",
    "unique_ids = set(dataset[\"_id\"])\n",
    "\n",
    "# Count of unique ids\n",
    "count_unique_ids = len(unique_ids)\n",
    "# print(count_unique_ids)‚\n",
    "\n",
    "for object_id, result in tqdm(processResults(dataset), total=count_unique_ids, desc=\"Uploading results\"):\n",
    "    pass\n",
    "    #updateProcessingResults(db, object_id, {\"denoising_result\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
