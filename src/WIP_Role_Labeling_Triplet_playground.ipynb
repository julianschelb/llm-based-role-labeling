{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to identify narratives in newspaper text is through considering the character archetypes relied on to compose the framing of an article. The main figures in an article may be represented as the heroes, villains, or victims in the text to guide the reader towards reading the article in context with existing qualities implicit in these character archetypes. Gomez-Zara et al present a dictionary-based method for computationally determining the hero, villain, and victim in a newspaper text, which Stammbach et al adapt by using an LLM for the same task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Articles (for Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials are sourced from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches a limited number of articles from the database that haven't been processed yet, \n",
    "returning specified fields like url, title, and parsing result text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"articles.sampled.triplets\"\n",
    "fields = {\"url\": 1, \"title\": 1, \"parsing_result.text\": 1}\n",
    "query = {\"triplets\": {\"$exists\": False}, \n",
    "         \"parsing_result.text_length\": {\"$lt\": 1000}}\n",
    "articles = fetchArticleTexts(db, 50, 0, fields, query, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Electronic Music Awards & Foundation Show 2016: Facts & Photos\n",
      "Text: Oliver Willis It looks like nothing was found at this location. Maybe try searching? AI Crime Watch\n"
     ]
    }
   ],
   "source": [
    "example_article = random.choice(articles)\n",
    "title = example_article.get(\"title\")\n",
    "text = example_article.get(\"parsing_result\").get(\"text\")\n",
    "print(f\"Title: {title}\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Processes the 'parsing_result' of each article to clean the text, and filters out articles \n",
    "that lack a 'title' or 'parsing_result'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning articles: 100%|██████████| 50/50 [00:00<00:00, 12360.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Basic text cleaning, e.g. removing newlines, tabs, etc.\n",
    "articles = cleanArticles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 50\n"
     ]
    }
   ],
   "source": [
    "# Filter out articles with no title or no parsing result \n",
    "articles = [article for article in articles if article.get(\n",
    "    \"title\", \"\") and article.get(\"parsing_result\", \"\")]\n",
    "\n",
    "print(\"Number of articles:\", len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the given data to a JSON file for optional visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportAsJSON(\"../data/input/articles.json\",  articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna-13B is an open-source chatbot developed by refining LLaMA through user-contributed conversations gathered from ShareGPT. Initial assessments employing GPT-4 as a referee indicate that Vicuna-13B attains over 90%* quality of OpenAI ChatGPT and Google Bard, surpassing other models such as LLaMA and Stanford Alpaca in over 90%* of instances. \n",
    "\n",
    "See:\n",
    "* https://github.com/lm-sys/FastChat\n",
    "* https://huggingface.co/lmsys/vicuna-13b-v1.5-16k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Start the controller service\n",
    "nohup python3 -m fastchat.serve.controller --host 0.0.0.0 --port 21001 &\n",
    "\n",
    "# Start the model_worker service\n",
    "nohup python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2 &\n",
    "nohup python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-33b-v1.3 --num-gpus 2 &\n",
    "\n",
    "# Start the gradio_web_server service\n",
    "nohup python3 -m fastchat.serve.gradio_web_server --host 0.0.0.0 --port 7860 &\n",
    "\n",
    "# Launch the RESTful API server\n",
    "nohup python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 8080 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f'Number of available GPUs: {num_gpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List infos about the available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n",
      "GPU 1:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpu_info_list = listAvailableGPUs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun Oct 22 15:51:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    34W / 250W |  13224MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    36W / 250W |  13224MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1792350      C   ...ions/fastchat/bin/python3    13222MiB |\n",
      "|    1   N/A  N/A   1792350      C   ...ions/fastchat/bin/python3    13222MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x7fccb25041d0> JSON: {\n",
      "  \"index\": 0,\n",
      "  \"text\": \"in a faraway land, there was a beautiful princess named Sophia.\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7fccb2504590> JSON: {\n",
      "  \"index\": 1,\n",
      "  \"text\": \", there was a young man who dreamed of becoming a great musician.\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7fccb2504040> JSON: {\n",
      "  \"index\": 2,\n",
      "  \"text\": \", there was a man called Jack. Jack was a simple man and lived a\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7fccb25047c0> JSON: {\n",
      "  \"index\": 3,\n",
      "  \"text\": \"in a small village, there was a young girl named Sophia. Sophia\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}, <OpenAIObject at 0x7fccb2504810> JSON: {\n",
      "  \"index\": 4,\n",
      "  \"text\": \"there was a little girl named Lily. She was a happy, curious and\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "model = RemoteModel(model_name=\"vicuna-13b-v1.5-16k\",\n",
    "                    api_base=\"http://merkur72.inf.uni-konstanz.de:8080/v1\",\n",
    "                    api_key=\"EMPTY\")\n",
    "\n",
    "params = {'n': 5}\n",
    "result = model.generateAnswers(\"Once upon a time\", params=params)\n",
    "print(result, type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {hero: \"Name\", villain: \"Name\", victim: \"Name\"}. \n",
      " Headline: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Text: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Solution: \n"
     ]
    }
   ],
   "source": [
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. A hero is an individual, organisation, or entity admired for their courage, noble qualities, and outstanding achievements. A villain is a character, organisation, or entity known for their wickedness or malicious actions, often serving as an antagonist in a story or narrative. A victim is an individual, organisation, or entity who suffers harm or adversity, often due to an external force or action. Every entity can only be one of those roles. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Every entity can only be one of those roles. If not existing return None as name. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Each entity can only assume one role. If none apply, use 'None'. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "PROMPT_TEMPLATE = \"Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. \\n Headline: '{headline}' \\n Text: '{article_text}' \\n Solution: \"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE.format(headline = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.', article_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(prompt_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "        'early_stopping': True,\n",
    "        # 'max_length': 100,\n",
    "        # 'min_length': 1,\n",
    "        'logprobs': 1,\n",
    "        'n': 3,\n",
    "        #'best_of': 1,\n",
    "        \n",
    "        'num_beam_groups': 2,\n",
    "        'num_beams': 5,\n",
    "        'num_return_sequences': 5,\n",
    "        'max_tokens': 50,\n",
    "        'min_tokens': 0,\n",
    "        'output_scores': True,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'temperature': 0.6,\n",
    "        'top_k': 50,\n",
    "        'top_p': 1.0 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTriplet(answer):\n",
    "    \"\"\" Extracts the triplet from the answer string. \"\"\"\n",
    "    \n",
    "    # Extract keys and values using regex\n",
    "    keys = re.findall(r'(\\w+):\\s*\\\"', answer)\n",
    "    values = re.findall(r'\\\"(.*?)\\\"', answer)\n",
    "    result = dict(zip(keys, values))\n",
    "\n",
    "    if result == {}:    \n",
    "        keys = re.findall(r'(\\w+):\\s*([^,]+)', answer)\n",
    "        result = dict((k, v.strip('\"')) for k, v in keys)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswersTriplets(article, model, template, params):\n",
    "    \"\"\" Generates answers for the given article using the model and template. \"\"\"\n",
    "\n",
    "    # Extract the article headline and text\n",
    "    article_headline=article.get(\"title\", \"\")\n",
    "    article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "    # Generate the answer\n",
    "    prompt = template.format(headline = article_headline, article_text = article_text)\n",
    "    answers = model.generateAnswers(prompt, params)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(text, n_tokens, tokenizer, overlap=10):\n",
    "    \"\"\"Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, \n",
    "    with an overlap of overlap tokens from the previous and the next chunks.\"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    # No previous chunk at the beginning, so no need for overlap\n",
    "    chunks.append(tokenizer.convert_tokens_to_string(tokens[i:i+n_tokens]))\n",
    "    i += n_tokens\n",
    "\n",
    "    while i < len(tokens):\n",
    "        # Now, we include overlap from the previous chunk\n",
    "        start_index = i - overlap\n",
    "        end_index = start_index + n_tokens\n",
    "        chunk = tokens[start_index:end_index]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        i += n_tokens - overlap  # Moving the index to account for the next overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(articles, model, template, params, chunk_size=1024, overlap=256, show_progress=False, verbose=False):\n",
    "    \"\"\"Processes a batch of articles and extracts the triplets.\"\"\"\n",
    "    runtimes = []  # List to store the runtime for each article\n",
    "\n",
    "    # Iterate over the articles\n",
    "    for article in tqdm(articles, desc=\"Generating answers\", disable=not show_progress):\n",
    "        start_time = time.time()  # Start the timer\n",
    "\n",
    "        # Extract the article headline and text\n",
    "        article_headline = article.get(\"title\", \"\")\n",
    "        article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "        # Split the article text into chunks\n",
    "        chunks = splitText(article_text, chunk_size,\n",
    "                            model.tokenizer, overlap=overlap)\n",
    "\n",
    "        # print(\"Chunks:\", len(chunks))\n",
    "\n",
    "        chunk_results = []\n",
    "        for chunk_id, chunk in enumerate(chunks):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Chunk:\", chunk_id)\n",
    "                print(\"Chunk Length:\", calcInputLength(model.tokenizer, chunk))\n",
    "\n",
    "            prompt = template.format(headline=article_headline, article_text=chunk)\n",
    "            answers = model.generateAnswers(prompt, params)\n",
    "\n",
    "            # Extract the triplet from seach answer\n",
    "            for answer in answers:\n",
    "                answer[\"triplet\"] = extractTriplet(answer.get(\"text\"))\n",
    "\n",
    "            results = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk\": chunk,\n",
    "                \"answers\": answers\n",
    "            }\n",
    "            chunk_results.append(results)\n",
    "\n",
    "        article[\"triplets\"] = chunk_results\n",
    "\n",
    "        end_time = time.time()  # End the timer\n",
    "        runtime = end_time - start_time  # Calculate the runtime\n",
    "        runtimes.append(runtime)  # Store the runtime\n",
    "\n",
    "    return articles, runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticle(db, id: str, values: dict = {}, collection=\"articles\"):\n",
    "    \"Updates scraping task in database\"\n",
    "    filter = {\"_id\": ObjectId(id)}\n",
    "    values = {\"$set\": {**values}}\n",
    "    r = db[collection].update_one(filter, values)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticles(db, articles, collection = \"articles\"):\n",
    "    \"\"\"Updates the articles in the database.\"\"\"\n",
    "\n",
    "    for article in tqdm(articles, desc=\"Uploading results\"):\n",
    "        id = article.get(\"_id\")\n",
    "        values = {\"triplets\": article.get(\"triplets\", [])}\n",
    "        updateArticle(db, id, values, collection) # TODO: Uncomment to update the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: Missouri Gov. Nixon to attend Detroit auto show\n",
      "Article Text: Biden doesn’t want to be president, so let’s help him with that Nails on a chalkboard: Biden’s disastrous trip to Maui GOP presidential contest has come down to Trump vs. DeSantis DETROIT (AP) - Misso\n"
     ]
    }
   ],
   "source": [
    "article = articles[40]\n",
    "print(\"Article Title:\", article.get(\"title\"))\n",
    "print(\"Article Text:\", article.get(\"parsing_result\").get(\"text\")[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Missouri Gov. N', 'Nixon to attend Detroit', 'Detroit auto show']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitText(article.get(\"title\"), 5, model.tokenizer, overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: >>> Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {hero: \"Name\", villain: \"Name\", victim: \"Name\"}. \n",
      " Headline: 'Missouri Gov. Nixon to attend Detroit auto show' \n",
      " Text: 'Biden doesn’t want to be president, so let’s help him with that Nails on a chalkboard: Biden’s disastrous trip to Maui GOP presidential contest has come down to Trump vs. DeSantis DETROIT (AP) - Missouri Gov. Jay Nixon is going to this year’s North American International Auto Show in Detroit. Nixon said in a statement that he plans to promote Missouri’s automotive industry while at the auto show Tuesday and Wednesday. Nixon touted what he called a comeback of the industry in Missouri in an announcement of his trip. He says he’ll meet with executives from Ford Motor Company and General Motors. This will be Nixon’s sixth year at the show as governor. Copyright © 2023 The Washington Times, LLC. Click to Read More and View Comments Click to Hide Terms of Use / Privacy Policy / Manage Newsletters' \n",
      " Solution:  <<<\n",
      "Prompt Input Length: 308\n"
     ]
    }
   ],
   "source": [
    "title = article.get(\"title\")\n",
    "text = article.get(\"parsing_result\").get(\"text\")\n",
    "prompt = PROMPT_TEMPLATE.format(headline =title, article_text = text)\n",
    "input_length = calcInputLength(model.tokenizer, prompt)\n",
    "\n",
    "print(\"Prompt: >>>\", prompt, \"<<<\")\n",
    "print(\"Prompt Input Length:\", input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: [<OpenAIObject at 0x7fccb2507290> JSON: {\n",
      "  \"index\": 0,\n",
      "  \"text\": \"\\n{hero: \\\"Jay Nixon\\\", villain: \\\"None\\\", victim: \\\"None\\\"}\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}, <OpenAIObject at 0x7fccb2506160> JSON: {\n",
      "  \"index\": 1,\n",
      "  \"text\": \"\\n{hero: \\\"Jay Nixon\\\", villain: \\\"None\\\", victim: \\\"None\\\"}\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}, <OpenAIObject at 0x7fccb2507f60> JSON: {\n",
      "  \"index\": 2,\n",
      "  \"text\": \"\\n{hero: \\\"Jay Nixon\\\", villain: \\\"None\\\", victim: \\\"None\\\"}\",\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "answer = getAnswersTriplets(article, model, PROMPT_TEMPLATE, params)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 5/5 [00:57<00:00, 11.50s/it]\n"
     ]
    }
   ],
   "source": [
    "articles, runtimes = processBatch(articles[:5], model, PROMPT_TEMPLATE, params, chunk_size = 1024, overlap= 64, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('64d8eb3a516b2658722949b1'),\n",
       " 'title': 'Trump Calls Kim Jong Un A ‘Maniac,’ Then Showers Him With Nausea-Inducing Praise',\n",
       " 'url': 'http://www.ifyouonlynews.com/politics/trump-calls-kim-jong-un-a-maniac-then-showers-him-with-nausea-inducing-praise/',\n",
       " 'parsing_result': {'text': 'View more information »'},\n",
       " 'triplets': [{'chunk_id': 0,\n",
       "   'chunk': 'View more information »',\n",
       "   'answers': [<OpenAIObject at 0x7fccb2507100> JSON: {\n",
       "      \"index\": 0,\n",
       "      \"text\": \"\\n{hero: None, villain: None, victim: None}\",\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"triplet\": {\n",
       "        \"hero\": \"None\",\n",
       "        \"villain\": \"None\",\n",
       "        \"victim\": \"None}\"\n",
       "      }\n",
       "    },\n",
       "    <OpenAIObject at 0x7fccb25040e0> JSON: {\n",
       "      \"index\": 1,\n",
       "      \"text\": \"\\n{hero: \\\"None\\\", villain: \\\"Donald Trump\\\", victim: \\\"North Korea\\\"}\\n\\nThe article reports on Donald Trump's conflicting statements about Kim Jong Un. On one hand, Trump called Kim a \\\"\",\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"triplet\": {\n",
       "        \"hero\": \"None\",\n",
       "        \"villain\": \"Donald Trump\",\n",
       "        \"victim\": \"North Korea\"\n",
       "      }\n",
       "    },\n",
       "    <OpenAIObject at 0x7fccb2507bf0> JSON: {\n",
       "      \"index\": 2,\n",
       "      \"text\": \"\\n {hero: \\\"None\\\", villain: \\\"Donald Trump\\\", victim: \\\"Kim Jong Un\\\"}\",\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"triplet\": {\n",
       "        \"hero\": \"None\",\n",
       "        \"villain\": \"Donald Trump\",\n",
       "        \"victim\": \"Kim Jong Un\"\n",
       "      }\n",
       "    }]}]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average runtime: 11.4986 seconds\n",
      "Standard Deviation of runtime: 1.4202 seconds\n"
     ]
    }
   ],
   "source": [
    "if runtimes:\n",
    "    avg_runtime = sum(runtimes) / len(runtimes)\n",
    "    print(f\"Average runtime: {avg_runtime:.4f} seconds\")\n",
    "else:\n",
    "    avg_runtime = 0\n",
    "\n",
    "if len(runtimes) > 1:\n",
    "    std_runtime = statistics.stdev(runtimes)\n",
    "    print(f\"Standard Deviation of runtime: {std_runtime:.4f} seconds\")\n",
    "else:\n",
    "    std_runtime = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10 # Number of articles to process in each batch\n",
    "CHUNK_SIZE = 20_000 # Number of tokens in each chunk\n",
    "OVERLAP = 64 # Number of overlapping tokens between chunks\n",
    "COLLECTION = \"articles.sampled.triplets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Batch 0 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 10/10 [01:44<00:00, 10.45s/it]\n",
      "Uploading results: 100%|██████████| 10/10 [00:00<00:00, 639.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 10 articles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"------ Batch {batch_id} ------\")\n",
    "\n",
    "    # Fetch the next batch of articles\n",
    "    articles = fetchArticleTexts(db, LIMIT, 0, fields, query, COLLECTION)\n",
    "    \n",
    "    # Stop if no more articles are available\n",
    "    if not articles:\n",
    "        break\n",
    "    \n",
    "    # Process the batch of articles\n",
    "    articles, runtimes = processBatch(articles, model, PROMPT_TEMPLATE, params, chunk_size=CHUNK_SIZE, overlap=OVERLAP, show_progress=True)\n",
    "\n",
    "    # Update the articles in the database\n",
    "    updateArticles(db, articles, COLLECTION)\n",
    "    print(f\"Updated {len(articles)} articles\", end=\"\\n\\n\")\n",
    "\n",
    "    batch_id += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('64d8eb3a516b2658722945bb'),\n",
       " 'title': 'Trump Spokesperson Channels Inner Death Eater: ‘Are There Any Pure Breeds Left’',\n",
       " 'url': 'http://www.ifyouonlynews.com/politics/trump-spokesperson-channels-inner-death-eater-are-there-any-pure-breeds-left/',\n",
       " 'parsing_result': {'text': 'View more information »'},\n",
       " 'triplets': [{'chunk_id': 0,\n",
       "   'chunk': 'View more information »',\n",
       "   'answers': [<OpenAIObject at 0x7fccb26a1030> JSON: {\n",
       "      \"index\": 0,\n",
       "      \"text\": \"\\n{hero: None, villain: \\\"Trump Spokesperson\\\", victim: \\\"Pure Breeds\\\"}\\n\\nExplanation:\\n\\n* The article does not mention any hero.\\n* The Trump Spokesp\",\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"triplet\": {\n",
       "        \"villain\": \"Trump Spokesperson\",\n",
       "        \"victim\": \"Pure Breeds\"\n",
       "      }\n",
       "    },\n",
       "    <OpenAIObject at 0x7fccb26a1490> JSON: {\n",
       "      \"index\": 1,\n",
       "      \"text\": \"\\n{hero: \\\"None\\\", villain: \\\"Katrina Pierson\\\", victim: \\\"None\\\"}\",\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"triplet\": {\n",
       "        \"hero\": \"None\",\n",
       "        \"villain\": \"Katrina Pierson\",\n",
       "        \"victim\": \"None\"\n",
       "      }\n",
       "    },\n",
       "    <OpenAIObject at 0x7fccb26a2250> JSON: {\n",
       "      \"index\": 2,\n",
       "      \"text\": \"\\n{hero: None, villain: Sarah Huckabee Sanders, victim: None}\\n\\nExplanation:\\n\\n* Sarah Huckabee Sanders, the White House Press Secretary, is the villain in\",\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"triplet\": {\n",
       "        \"hero\": \"None\",\n",
       "        \"villain\": \"Sarah Huckabee Sanders\",\n",
       "        \"victim\": \"None}\\n\\nExplanation:\\n\\n* Sarah Huckabee Sanders\"\n",
       "      }\n",
       "    }]}]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Stopped before updating the database!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stopped before updating the database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"Stopped before updating the database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Results to old Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch articles from database\n",
    "fields = {\"triplets\": 1}\n",
    "query = {\"triplets\": {\"$exists\": True}}\n",
    "articles = fetchArticleTexts(db,  limit=0, skip=0, fields=fields, query=query, collection=\"articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_answers = [\n",
    "    \"None\",\n",
    "    \"'None'\",\n",
    "    \"'none'\",\n",
    "    \"None}\",\n",
    "    \"{None\",\n",
    "    \"Not applicable\",\n",
    "    \"Not available\",\n",
    "    \"Not specified\",\n",
    "    \"No data\",\n",
    "    \"No value\",\n",
    "    \"Invalid\",\n",
    "    \"Unspecified\",\n",
    "    \"Empty\",\n",
    "    \"Missing\",\n",
    "    \"Null\",\n",
    "    \"Undefined\",\n",
    "    \"N/A\",\n",
    "    \"NA\",\n",
    "    \"Not provided\",\n",
    "    \"No information\",\n",
    "    \"Not set\",\n",
    "    \"No entry\",\n",
    "    \"No response\",\n",
    "    \"Not applicable\",\n",
    "    \"Not determined\",\n",
    "    \"No result\",\n",
    "    \"No answer\",\n",
    "    \"No record\",\n",
    "    \"No match\",\n",
    "    \"No selection\",\n",
    "    \"Not found\",\n",
    "    \"Not valid\",\n",
    "    \"Not given\",\n",
    "    \"Not filled\",\n",
    "    \"Not assigned\",\n",
    "    \"No choice\",\n",
    "    \"Not used\",\n",
    "    \"No sample\",\n",
    "    \"Not measured\",\n",
    "    \"No response\",\n",
    "    \"Not reported\",\n",
    "    \"Not registered\",\n",
    "    \"Not logged\",\n",
    "    \"No feedback\",\n",
    "    \"No score\",\n",
    "    \"No grade\",\n",
    "    \"No rating\",\n",
    "    \"No rating available\",\n",
    "    \"No rating provided\",\n",
    "    \"No rating assigned\",\n",
    "    \"No rating given\",\n",
    "    \"No rating received\",\n",
    "    \"No rating found\",\n",
    "    \"No rating available\",\n",
    "    \"No rating recorded\",\n",
    "    \"No rating obtained\",\n",
    "    \"No rating submitted\",\n",
    "    \"No rating included\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNone(input_string, alternative_names):\n",
    "    \"\"\"Checks if the input string contains one of the alternative names.\"\"\"\n",
    "    \n",
    "    for name in alternative_names:\n",
    "        if name == input_string.strip():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = db[COLLECTION].update_many({}, {\"$unset\": {\"triplets\": \"\"}})\n",
    "#result = db.articles.sampled.triplets.update_many({}, {\"$unset\": {\"processing_result\": \"\"}})\n",
    "#result = db.articles.sampled.triplets.update_many({}, {\"$unset\": {\"denoising_result\": \"\"}})\n",
    "#result = db.articles.sampled.triplets.update_many({}, {\"$unset\": {\"embedding_result\": \"\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all documents in the collection\n",
    "for article in tqdm(articles, desc=\"Uploading results\"):\n",
    "    chunks = article.get(\"triplets\", [])  # Get the \"triplets\" property\n",
    "\n",
    "    #print(chunks)\n",
    "\n",
    "    heros, villains, victims = [], [], []\n",
    "\n",
    "    # Extract data from the \"triplets\" property\n",
    "    for chunk in chunks:\n",
    "\n",
    "\n",
    "        triplet = chunk.get(\"triplet\", {})\n",
    "        hero = triplet.get(\"hero\", \"None\")\n",
    "        villain = triplet.get(\"villain\", \"None\")\n",
    "        victim = triplet.get(\"victim\", \"None\")\n",
    "\n",
    "        #print(hero, villain, victim)\n",
    "\n",
    "        if not isNone(hero, invalid_answers):\n",
    "            heros.append(hero)\n",
    "        if not isNone(villain, invalid_answers):\n",
    "            villains.append(villain)\n",
    "        if not isNone(victim, invalid_answers):\n",
    "            victims.append(victim)\n",
    "\n",
    "    # Create the processing_result structure\n",
    "    processing_result = {\n",
    "        \"hero\": heros,\n",
    "        \"villain\": villains,\n",
    "        \"victim\": victims\n",
    "    }\n",
    "   \n",
    "    #print(processing_result)\n",
    "    \n",
    "    # Update the document in the database   \n",
    "    id = article.get(\"_id\")\n",
    "    values = {\"processing_result\": processing_result}\n",
    "    updateArticle(db, id, values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
