{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to identify narratives in newspaper text is through considering the character archetypes relied on to compose the framing of an article. The main figures in an article may be represented as the heroes, villains, or victims in the text to guide the reader towards reading the article in context with existing qualities implicit in these character archetypes. Gomez-Zara et al present a dictionary-based method for computationally determining the hero, villain, and victim in a newspaper text, which Stammbach et al adapt by using an LLM for the same task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Articles (for Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials are sourced from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches a limited number of articles from the database that haven't been processed yet, \n",
    "returning specified fields like url, title, and parsing result text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"articles\"\n",
    "fields = {\"url\": 1, \"title\": 1, \"parsing_result.text\": 1}\n",
    "query = {\"processing_result\": {\"$exists\": False}, \n",
    "         \"parsing_result.text_length\": {\"$lt\": 10000}}\n",
    "articles = fetchArticleTexts(db, 50, 0, fields, query, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 'Buckle up': Princeton professor warns that political violence is 'going to get worse'\n",
      "Text: Political violence in the United States is continuing to get worse and with easy access to weapons of mass death, there is a fear that it could generate larger body counts. Princeton Professor Eddie Glaude warned this is just the beginning of MSNBC's The Last Word on Monday evening. \"It reveals, the corruption of the heart. They've turned some of Americans into more monsters. It reveals we have political nihilists who are part of a major political party, who value nothing and no, except for holding power\" said Glaude. \"And you combine those political nihilists with cynical people and people who are motivated by hate, and you get violence.\" Glaude went on to say the problem isn't just rhetoric. \"I'm thinking very quickly of two historical moments,\" he recalled. \"Moments where there seems to be an existential threat that defined how Americans came to blows with each other. The mid-20th century, and of course the mid-19th century. The 1850s, and 1950s and 1960s. and there was a clear line drawn, and violence to find the period. We need to understand that we're in a moment that's not just about, you know, toning down the rhetoric. There are those who believe, that their way of life is existentially threatened, and they are going to defend it at all costs. We need to understand where we are as a nation what the midterms represent, and what is ahead of us. I think we all need to buckle up, Lawrence. It's gonna get worse.\" See the full comments below or at this link: It's going to get worsewww.youtube.com The GOP hypocrisy surrounding Donald Trump is on another level, a Democratic strategist said on Saturday. Kurt Bardella, a former Republican operative, appeared on MSNBC's Ayman on Saturday to give his thoughts on the cognitive dissonance present within the modern GOP.        Donald Trump is holding the Republican Party \"hostage,\" forcing GOPers to stick with him when they don't want to, the former president's biographer said Saturday. During an appearance on MSNBC's Ayman on Saturday night, Trump biographer and Bloomberg Opinion executive editor Timothy O'Brien was asked about Trump's rivals in the primary race for the 2024 Republican presidential nomination. Specifically, O'Brien was asked if anyone would be taking the fight directly to Trump.        Republicans are giving citizens understandable whiplash from their messaging in connection with Hunter Biden,  former Rep. David Jolly (R-FL) said on MSNBC Saturday. Jolly, who recently said voters are rejecting Republican extremism in record numbers, appeared on American Voices with Alicia Menendez where he was asked about the political ramifications of Republicans demanding a special counsel to investigate Hunter Biden, only to call the appointment of one a \"sham.\"       Copyright © 2023 Raw Story Media, Inc. PO Box 21050, Washington, D.C. 20009 |\n",
      "    Masthead  |\n",
      "    Privacy Policy  |\n",
      "    Manage Preferences | Debug Logs\n",
      "    For corrections contact\n",
      "    corrections@rawstory.com, for support contact\n",
      "    support@rawstory.com.\n"
     ]
    }
   ],
   "source": [
    "example_article = random.choice(articles)\n",
    "title = example_article.get(\"title\")\n",
    "text = example_article.get(\"parsing_result\").get(\"text\")\n",
    "print(f\"Title: {title}\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Processes the 'parsing_result' of each article to clean the text, and filters out articles \n",
    "that lack a 'title' or 'parsing_result'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning articles: 100%|██████████| 50/50 [00:00<00:00, 4354.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Basic text cleaning, e.g. removing newlines, tabs, etc.\n",
    "articles = cleanArticles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 50\n"
     ]
    }
   ],
   "source": [
    "# Filter out articles with no title or no parsing result \n",
    "articles = [article for article in articles if article.get(\n",
    "    \"title\", \"\") and article.get(\"parsing_result\", \"\")]\n",
    "\n",
    "print(\"Number of articles:\", len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the given data to a JSON file for optional visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportAsJSON(\"../data/input/articles.json\",  articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna-13B is an open-source chatbot developed by refining LLaMA through user-contributed conversations gathered from ShareGPT. Initial assessments employing GPT-4 as a referee indicate that Vicuna-13B attains over 90%* quality of OpenAI ChatGPT and Google Bard, surpassing other models such as LLaMA and Stanford Alpaca in over 90%* of instances. \n",
    "\n",
    "See:\n",
    "* https://github.com/lm-sys/FastChat\n",
    "* https://huggingface.co/lmsys/vicuna-13b-v1.5-16k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Start the controller service\n",
    "nohup python3 -m fastchat.serve.controller --host 0.0.0.0 --port 21001 &\n",
    "\n",
    "# Start the model_worker service\n",
    "nohup python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2 &\n",
    "\n",
    "# Start the gradio_web_server service\n",
    "nohup python3 -m fastchat.serve.gradio_web_server --host 0.0.0.0 --port 7860 &\n",
    "\n",
    "# Launch the RESTful API server\n",
    "nohup python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 8080 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f'Number of available GPUs: {num_gpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List infos about the available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n",
      "GPU 1:\n",
      "  Name: Tesla P100-PCIE-16GB\n",
      "  Memory: 16276.00 MiB\n",
      "  Compute Capability: 6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpu_info_list = listAvailableGPUs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 21 20:04:21 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    34W / 250W |  13224MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    36W / 250W |  13224MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1235841      C   ...ions/fastchat/bin/python3    13222MiB |\n",
      "|    1   N/A  N/A   1235841      C   ...ions/fastchat/bin/python3    13222MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", there was a guy named Warren. He was a pretty regular guy\n"
     ]
    }
   ],
   "source": [
    "model = RemoteModel(model_name=\"vicuna-13b-v1.5-16k\",\n",
    "                    api_base=\"http://merkur72.inf.uni-konstanz.de:8080/v1\",\n",
    "                    api_key=\"EMPTY\")\n",
    "\n",
    "print(model.generateAnswer(\"Once upon a time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {hero: \"Name\", villain: \"Name\", victim: \"Name\"}. \n",
      " Headline: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Text: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' \n",
      " Solution: \n"
     ]
    }
   ],
   "source": [
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. A hero is an individual, organisation, or entity admired for their courage, noble qualities, and outstanding achievements. A villain is a character, organisation, or entity known for their wickedness or malicious actions, often serving as an antagonist in a story or narrative. A victim is an individual, organisation, or entity who suffers harm or adversity, often due to an external force or action. Every entity can only be one of those roles. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Every entity can only be one of those roles. If not existing return None as name. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "# PROMPT_TEMPLATE = \"Please identify entities which are portrayed as hero, villain and victim in the following news article. Each entity can only assume one role. If none apply, use 'None'. The solution must be returned in this format {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. Article Headline: ''{headline}''. Article Text: ''{article_text}''  Solution: \"\n",
    "\n",
    "PROMPT_TEMPLATE = \"Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {{hero: \\\"Name\\\", villain: \\\"Name\\\", victim: \\\"Name\\\"}}. \\n Headline: '{headline}' \\n Text: '{article_text}' \\n Solution: \"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE.format(headline = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.', article_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(prompt_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "        'early_stopping': True,\n",
    "        # 'max_length': 100,\n",
    "        # 'min_length': 1,\n",
    "        'logprobs': 1,\n",
    "        'n': 1,\n",
    "        'best_of': 1,\n",
    "        \n",
    "        'num_beam_groups': 2,\n",
    "        'num_beams': 5,\n",
    "        'num_return_sequences': 5,\n",
    "        'max_tokens': 50,\n",
    "        'min_tokens': 0,\n",
    "        'output_scores': True,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'temperature': 0.6,\n",
    "        'top_k': 50,\n",
    "        'top_p': 1.0 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTriplet(answer):\n",
    "    \"\"\" Extracts the triplet from the answer string. \"\"\"\n",
    "    \n",
    "    # Extract keys and values using regex\n",
    "    keys = re.findall(r'(\\w+):\\s*\\\"', answer)\n",
    "    values = re.findall(r'\\\"(.*?)\\\"', answer)\n",
    "    result = dict(zip(keys, values))\n",
    "\n",
    "    if result == {}:    \n",
    "        keys = re.findall(r'(\\w+):\\s*([^,]+)', answer)\n",
    "        result = dict((k, v.strip('\"')) for k, v in keys)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswersTriplets(article, model, template, params):\n",
    "    \"\"\" Generates answers for the given article using the model and template. \"\"\"\n",
    "\n",
    "    # Extract the article headline and text\n",
    "    article_headline=article.get(\"title\", \"\")\n",
    "    article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "    # Generate the answer\n",
    "    prompt = template.format(headline = article_headline, article_text = article_text)\n",
    "    answer = model.generateAnswer(prompt, params)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(text, n_tokens, tokenizer, overlap=10):\n",
    "    \"\"\"Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, \n",
    "    with an overlap of overlap tokens from the previous and the next chunks.\"\"\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    # No previous chunk at the beginning, so no need for overlap\n",
    "    chunks.append(tokenizer.convert_tokens_to_string(tokens[i:i+n_tokens]))\n",
    "    i += n_tokens\n",
    "\n",
    "    while i < len(tokens):\n",
    "        # Now, we include overlap from the previous chunk\n",
    "        start_index = i - overlap\n",
    "        end_index = start_index + n_tokens\n",
    "        chunk = tokens[start_index:end_index]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        i += n_tokens - overlap  # Moving the index to account for the next overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(articles, model, template, params, chunk_size=1024, overlap=256, show_progress=False, verbose=False):\n",
    "    \"\"\"Processes a batch of articles and extracts the triplets.\"\"\"\n",
    "    runtimes = []  # List to store the runtime for each article\n",
    "\n",
    "    # Iterate over the articles\n",
    "    for article in tqdm(articles, desc=\"Generating answers\", disable=not show_progress):\n",
    "        start_time = time.time()  # Start the timer\n",
    "\n",
    "        # Extract the article headline and text\n",
    "        article_headline = article.get(\"title\", \"\")\n",
    "        article_text = article.get(\"parsing_result\").get(\"text\")\n",
    "\n",
    "        # Split the article text into chunks\n",
    "        chunks = splitText(article_text, chunk_size,\n",
    "                            model.tokenizer, overlap=overlap)\n",
    "\n",
    "        # print(\"Chunks:\", len(chunks))\n",
    "\n",
    "        chunk_results = []\n",
    "        for chunk_id, chunk in enumerate(chunks):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Chunk:\", chunk_id)\n",
    "                print(\"Chunk Length:\", calcInputLength(model.tokenizer, chunk))\n",
    "            # print(\"Headline Length:\", calcInputLength(\n",
    "            #     tokenizer, article_headline))\n",
    "\n",
    "            prompt = template.format(\n",
    "                headline=article_headline, article_text=chunk)\n",
    "            answer = model.generateAnswer(prompt, params)\n",
    "            triplet = extractTriplet(answer)\n",
    "\n",
    "            # print(chunk_id, \"Answer:\", triplet, \"Type:\", type(triplet))\n",
    "            results = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk\": chunk,\n",
    "                \"answer\": answer,\n",
    "                \"triplet\": triplet\n",
    "            }\n",
    "            chunk_results.append(results)\n",
    "\n",
    "        article[\"triplets\"] = chunk_results\n",
    "\n",
    "        end_time = time.time()  # End the timer\n",
    "        runtime = end_time - start_time  # Calculate the runtime\n",
    "        runtimes.append(runtime)  # Store the runtime\n",
    "\n",
    "    return articles, runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticle(db, id: str, values: dict = {}, collection=\"articles\"):\n",
    "    \"Updates scraping task in database\"\n",
    "    pass # TODO: Uncomment to update the database\n",
    "    # filter = {\"_id\": ObjectId(id)}\n",
    "    # values = {\"$set\": {**values}}\n",
    "    # r = db[collection].update_one(filter, values)\n",
    "    # return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArticles(db, articles):\n",
    "    \"\"\"Updates the articles in the database.\"\"\"\n",
    "\n",
    "    for article in tqdm(articles, desc=\"Uploading results\"):\n",
    "        id = article.get(\"_id\")\n",
    "        values = {\"triplets\": article.get(\"triplets\", [])}\n",
    "        #updateArticle(db, id, values) # TODO: Uncomment to update the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: GOP threats to Medicare and Social Security take center stage in last days before election\n",
      "Article Text: According to a report from CNN, President Joe Biden is headed to Florida on Tuesday where he will lead the charge against Republicans who want to gut Medicare and dismantle Social Security in a state \n"
     ]
    }
   ],
   "source": [
    "article = articles[40]\n",
    "print(\"Article Title:\", article.get(\"title\"))\n",
    "print(\"Article Text:\", article.get(\"parsing_result\").get(\"text\")[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GOP threats to',\n",
       " 'to Medicare and Social',\n",
       " 'Social Security take center stage',\n",
       " 'stage in last days before',\n",
       " 'before election']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitText(article.get(\"title\"), 5, model.tokenizer, overlap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: >>> Given the news article below, identify entities categorized as a hero, villain, or victim. Each entity can only assume one role. If none apply, use 'None'. The solution must be provided in this format: {hero: \"Name\", villain: \"Name\", victim: \"Name\"}. \n",
      " Headline: 'GOP threats to Medicare and Social Security take center stage in last days before election' \n",
      " Text: 'According to a report from CNN, President Joe Biden is headed to Florida on Tuesday where he will lead the charge against Republicans who want to gut Medicare and dismantle Social Security in a state that is up to its ears in retirees. With one week before the midterm election, Biden will warn voters that the GOP -- led by Sens. Rick Scott (R-FL) and Ron Johnson (R-WI) -- will destroy the social safety net if given the chance while painting them as \"extreme MAGA Republicans.” \"Among his chief foils is Scott, the head of Republicans’ campaign arm who had laid out a policy agenda that would put Medicare, Social Security and other government programs up for a vote every five years,\" CNN is reporting. \"The state is also home to former President Donald Trump and Gov. Ron DeSantis, both likely 2024 presidential candidates whom Democrats have been eager to cast as the faces of a new, more extreme Republican Party.\" According to a Biden adviser, the president thinks the Republican party is vulnerable on the issue of the security of Social Security and Medicare. IN OTHER NEWS: Doug Mastriano controls Facebook page that pumps out right-wing nonsense “As the congressional Republican plan to either eliminate Social Security and Medicare, cut Social Security and Medicare or hold it hostage to debt limit negotiations becomes even more apparent…it’s even more relevant for the President to draw that choice for the voters of Florida and the voters across the country,” the senior adviser explained. You can read more here. The GOP hypocrisy surrounding Donald Trump is on another level, a Democratic strategist said on Saturday. Kurt Bardella, a former Republican operative, appeared on MSNBC's Ayman on Saturday to give his thoughts on the cognitive dissonance present within the modern GOP. Donald Trump is holding the Republican Party \"hostage,\" forcing GOPers to stick with him when they don't want to, the former president's biographer said Saturday. During an appearance on MSNBC's Ayman on Saturday night, Trump biographer and Bloomberg Opinion executive editor Timothy O'Brien was asked about Trump's rivals in the primary race for the 2024 Republican presidential nomination. Specifically, O'Brien was asked if anyone would be taking the fight directly to Trump. Republicans are giving citizens understandable whiplash from their messaging in connection with Hunter Biden, former Rep. David Jolly (R-FL) said on MSNBC Saturday. Jolly, who recently said voters are rejecting Republican extremism in record numbers, appeared on American Voices with Alicia Menendez where he was asked about the political ramifications of Republicans demanding a special counsel to investigate Hunter Biden, only to call the appointment of one a \"sham.\" Copyright © 2023 Raw Story Media, Inc. PO Box 21050, Washington, D.C. 20009 | Masthead | Privacy Policy | Manage Preferences | Debug Logs For corrections contact corrections@rawstory.com, for support contact support@rawstory.com.' \n",
      " Solution:  <<<\n",
      "Prompt Input Length: 815\n"
     ]
    }
   ],
   "source": [
    "title = article.get(\"title\")\n",
    "text = article.get(\"parsing_result\").get(\"text\")\n",
    "prompt = PROMPT_TEMPLATE.format(headline =title, article_text = text)\n",
    "input_length = calcInputLength(model.tokenizer, prompt)\n",
    "\n",
    "print(\"Prompt: >>>\", prompt, \"<<<\")\n",
    "print(\"Prompt Input Length:\", input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "{hero: \"Joe Biden\", villain: \"Rick Scott, Ron Johnson\", victim: \"Republican Party\"}\n"
     ]
    }
   ],
   "source": [
    "answer = getAnswersTriplets(article, model, PROMPT_TEMPLATE, params)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 5/5 [00:38<00:00,  7.67s/it]\n"
     ]
    }
   ],
   "source": [
    "articles, runtimes = processBatch(articles[:5], model, PROMPT_TEMPLATE, params, chunk_size = 1024, overlap= 64, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('64d8e926516b265872e629f2'),\n",
       " 'title': 'Chicago police say up to fourteen people may have been shot nine critically injured on West Side',\n",
       " 'url': 'https://biztoc.com/p/fgcuuxii?ref=rss&rd=1',\n",
       " 'parsing_result': {'text': \"Key events: 18': Luis Diaz opens the scoring for Liverpool, converting a great early cross from Salah. 29': Mohamed Salah appears to have... Chelsea are taking on Liverpool in their Premier League opener at Stamford Bridge. The score is currently level 1-1 in the second half. Tottenham began life without Harry Kane with an entertaining 2-2 draw at Brentford in the Premier League on Sunday. The Chiefs and Saints are facing off in the 2023 NFL preseason. We break down how to watch the matchup. A quick look at the Jaguars' 28-23 victory over the Dallas Cowboys in a 2023 Preseason Week 1 game at AT&T Stadium in Arlington, Texas, Saturday. Eagles linebacker and special teams ace Shaun Bradley was carted off the field in the third quarter of the team's preseason opener... The Carolina Panthers' offensive line took responsibility for Bryce Young's struggles in his preseason debut after the rookie was hit... Arts and culture Disasters and accidents Politics and elections Armed conflicts and attacks Disasters and accidents Law and crime Politics and elections Sports\"},\n",
       " 'triplets': [{'chunk_id': 0,\n",
       "   'chunk': \"Key events: 18': Luis Diaz opens the scoring for Liverpool, converting a great early cross from Salah. 29': Mohamed Salah appears to have... Chelsea are taking on Liverpool in their Premier League opener at Stamford Bridge. The score is currently level 1-1 in the second half. Tottenham began life without Harry Kane with an entertaining 2-2 draw at Brentford in the Premier League on Sunday. The Chiefs and Saints are facing off in the 2023 NFL preseason. We break down how to watch the matchup. A quick look at the Jaguars' 28-23 victory over the Dallas Cowboys in a 2023 Preseason Week 1 game at AT&T Stadium in Arlington, Texas, Saturday. Eagles linebacker and special teams ace Shaun Bradley was carted off the field in the third quarter of the team's preseason opener... The Carolina Panthers' offensive line took responsibility for Bryce Young's struggles in his preseason debut after the rookie was hit... Arts and culture Disasters and accidents Politics and elections Armed conflicts and attacks Disasters and accidents Law and crime Politics and elections Sports\",\n",
       "   'answer': '\\n{hero: \"Luis Diaz\", villain: \"Mohamed Salah\", victim: \"Shaun Bradley\"}',\n",
       "   'triplet': {'hero': 'Luis Diaz',\n",
       "    'villain': 'Mohamed Salah',\n",
       "    'victim': 'Shaun Bradley'}}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average runtime: 7.6656 seconds\n",
      "Standard Deviation of runtime: 6.6863 seconds\n"
     ]
    }
   ],
   "source": [
    "if runtimes:\n",
    "    avg_runtime = sum(runtimes) / len(runtimes)\n",
    "    print(f\"Average runtime: {avg_runtime:.4f} seconds\")\n",
    "else:\n",
    "    avg_runtime = 0\n",
    "\n",
    "if len(runtimes) > 1:\n",
    "    std_runtime = statistics.stdev(runtimes)\n",
    "    print(f\"Standard Deviation of runtime: {std_runtime:.4f} seconds\")\n",
    "else:\n",
    "    std_runtime = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10 # Number of articles to process in each batch\n",
    "CHUNK_SIZE = 1024 # Number of tokens in each chunk\n",
    "OVERLAP = 64 # Number of overlapping tokens between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Batch 0 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 10/10 [01:07<00:00,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 10 articles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"------ Batch {batch_id} ------\")\n",
    "\n",
    "    # Fetch the next batch of articles\n",
    "    articles = fetchArticleTexts(db, LIMIT, 0, fields, query)\n",
    "    \n",
    "    # Stop if no more articles are available\n",
    "    if not articles:\n",
    "        break\n",
    "    \n",
    "    # Process the batch of articles\n",
    "    articles, runtimes = processBatch(articles, model, PROMPT_TEMPLATE, params, chunk_size=CHUNK_SIZE, overlap=OVERLAP, show_progress=True)\n",
    "\n",
    "    # Update the articles in the database\n",
    "    #updateArticles(db, articles)\n",
    "    print(f\"Updated {len(articles)} articles\", end=\"\\n\\n\")\n",
    "\n",
    "    batch_id += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Stopped before updating the database!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stopped before updating the database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"Stopped before updating the database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Results to old Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch articles from database\n",
    "fields = {\"triplets\": 1}\n",
    "query = {\"triplets\": {\"$exists\": True}}\n",
    "articles = fetchArticleTexts(db,  limit=0, skip=0, fields=fields, query=query, collection=\"articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_answers = [\n",
    "    \"None\",\n",
    "    \"'None'\",\n",
    "    \"'none'\",\n",
    "    \"None}\",\n",
    "    \"{None\",\n",
    "    \"Not applicable\",\n",
    "    \"Not available\",\n",
    "    \"Not specified\",\n",
    "    \"No data\",\n",
    "    \"No value\",\n",
    "    \"Invalid\",\n",
    "    \"Unspecified\",\n",
    "    \"Empty\",\n",
    "    \"Missing\",\n",
    "    \"Null\",\n",
    "    \"Undefined\",\n",
    "    \"N/A\",\n",
    "    \"NA\",\n",
    "    \"Not provided\",\n",
    "    \"No information\",\n",
    "    \"Not set\",\n",
    "    \"No entry\",\n",
    "    \"No response\",\n",
    "    \"Not applicable\",\n",
    "    \"Not determined\",\n",
    "    \"No result\",\n",
    "    \"No answer\",\n",
    "    \"No record\",\n",
    "    \"No match\",\n",
    "    \"No selection\",\n",
    "    \"Not found\",\n",
    "    \"Not valid\",\n",
    "    \"Not given\",\n",
    "    \"Not filled\",\n",
    "    \"Not assigned\",\n",
    "    \"No choice\",\n",
    "    \"Not used\",\n",
    "    \"No sample\",\n",
    "    \"Not measured\",\n",
    "    \"No response\",\n",
    "    \"Not reported\",\n",
    "    \"Not registered\",\n",
    "    \"Not logged\",\n",
    "    \"No feedback\",\n",
    "    \"No score\",\n",
    "    \"No grade\",\n",
    "    \"No rating\",\n",
    "    \"No rating available\",\n",
    "    \"No rating provided\",\n",
    "    \"No rating assigned\",\n",
    "    \"No rating given\",\n",
    "    \"No rating received\",\n",
    "    \"No rating found\",\n",
    "    \"No rating available\",\n",
    "    \"No rating recorded\",\n",
    "    \"No rating obtained\",\n",
    "    \"No rating submitted\",\n",
    "    \"No rating included\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNone(input_string, alternative_names):\n",
    "    \"\"\"Checks if the input string contains one of the alternative names.\"\"\"\n",
    "    \n",
    "    for name in alternative_names:\n",
    "        if name == input_string.strip():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = db.articles.update_many({}, {\"$unset\": {\"embedding_result_tokens\": \"\"}})\n",
    "#result = db.articles.sampled.triplets.update_many({}, {\"$unset\": {\"processing_result\": \"\"}})\n",
    "#result = db.articles.sampled.triplets.update_many({}, {\"$unset\": {\"denoising_result\": \"\"}})\n",
    "#result = db.articles.sampled.triplets.update_many({}, {\"$unset\": {\"embedding_result\": \"\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all documents in the collection\n",
    "for article in tqdm(articles, desc=\"Uploading results\"):\n",
    "    chunks = article.get(\"triplets\", [])  # Get the \"triplets\" property\n",
    "\n",
    "    #print(chunks)\n",
    "\n",
    "    heros, villains, victims = [], [], []\n",
    "\n",
    "    # Extract data from the \"triplets\" property\n",
    "    for chunk in chunks:\n",
    "\n",
    "\n",
    "        triplet = chunk.get(\"triplet\", {})\n",
    "        hero = triplet.get(\"hero\", \"None\")\n",
    "        villain = triplet.get(\"villain\", \"None\")\n",
    "        victim = triplet.get(\"victim\", \"None\")\n",
    "\n",
    "        #print(hero, villain, victim)\n",
    "\n",
    "        if not isNone(hero, invalid_answers):\n",
    "            heros.append(hero)\n",
    "        if not isNone(villain, invalid_answers):\n",
    "            villains.append(villain)\n",
    "        if not isNone(victim, invalid_answers):\n",
    "            victims.append(victim)\n",
    "\n",
    "    # Create the processing_result structure\n",
    "    processing_result = {\n",
    "        \"hero\": heros,\n",
    "        \"villain\": villains,\n",
    "        \"victim\": victims\n",
    "    }\n",
    "   \n",
    "    #print(processing_result)\n",
    "    \n",
    "    # Update the document in the database   \n",
    "    id = article.get(\"_id\")\n",
    "    values = {\"processing_result\": processing_result}\n",
    "    updateArticle(db, id, values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
